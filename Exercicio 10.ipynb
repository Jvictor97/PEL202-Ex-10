{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learn para o mundo de grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando a estrutura do mundo das grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parâmetros para desenhar o mundo de grades**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "\n",
    "master = None\n",
    "size = 100\n",
    "player = (0, 1)\n",
    "max_x = 3\n",
    "max_y = 2\n",
    "final = (2,0)\n",
    "restart = False\n",
    "board = None\n",
    "me = None\n",
    "r = 30\n",
    "arrow_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para desenhar o tabuleiro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_board():\n",
    "    global board, me, master, r\n",
    "    \n",
    "    # Cria a janela\n",
    "    master = Tk()\n",
    "    board = Canvas(master, width=3*size, height=2*size) \n",
    "\n",
    "    # Desenha as grades\n",
    "    for row in range(max_x):\n",
    "        for column in range(max_y):\n",
    "            board.create_rectangle(row*size, column*size, (row+1)*size, (column+1)*size, fill=\"white\", width=1)  \n",
    "\n",
    "    # Desenha o objetivo em verde            \n",
    "    board.create_rectangle(final[0]*size, final[1]*size, (final[0]+1)*size, (final[1]+1)*size, fill=\"green\", width=1)\n",
    "\n",
    "    # Define as coordenadas iniciais do agente\n",
    "    x0 = player[0]*size + size / 2 - r\n",
    "    y0 = player[1]*size + size / 2 - r\n",
    "    x1 = player[0]*size + size / 2 + r\n",
    "    y1 = player[1]*size + size / 2 + r\n",
    "\n",
    "    # Desenha o agente no tabuleiro\n",
    "    me = board.create_oval(x0, y0, x1, y1, fill=\"red\", width=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para movimentar o agente no tabuleiro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(dx, dy): # Recebe delta X e delta Y\n",
    "    global player, me, restart, board\n",
    "    \n",
    "    # Atualiza as coordenadas temporárias do agente\n",
    "    new_x = player[0] + dx\n",
    "    new_y = player[1] + dy\n",
    "    score = 0 # Define o score inicialmente como zero\n",
    "    \n",
    "    # Verifica se a nova posição do agente é válida\n",
    "    if new_x >= 0 and new_y >= 0 and new_x < max_x and new_y < max_y:\n",
    "        x0 = new_x*size + size / 2 - r\n",
    "        y0 = new_y*size + size / 2 - r\n",
    "        x1 = new_x*size + size / 2 + r\n",
    "        y1 = new_y*size + size / 2 + r\n",
    "\n",
    "        board.coords(me, x0, y0, x1, y1)\n",
    "        player = (new_x, new_y)\n",
    "    else: # Caso não seja, score será -1\n",
    "        score = -1\n",
    "        \n",
    "    # Caso a nova posição do agente seja o estado final\n",
    "    if (new_x, new_y) == final:\n",
    "        score = 1 # Score será +1\n",
    "        restart = True # Flag para reiniciar a posição do agente\n",
    "    \n",
    "    return score # Retorna o valor de score\n",
    "\n",
    "# Função que inicia o tkinter\n",
    "def begin():\n",
    "    global master, board\n",
    "    create_board()\n",
    "    board.grid(row=0, column=0)\n",
    "    master.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funções para movimentar o agente no mundo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_up(event=None):\n",
    "    return move(0, -1)\n",
    "\n",
    "def move_down(event=None):\n",
    "    return move(0, 1)\n",
    "\n",
    "def move_left(event=None):\n",
    "    return move(-1, 0)\n",
    "\n",
    "def move_right(event=None):\n",
    "    return move(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parâmetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "actions = [move_up, move_down, move_left, move_right]\n",
    "alpha = 1\n",
    "gamma = 0.9\n",
    "score = 1\n",
    "t = 1\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Return(Enum):\n",
    "    ACTION = 0\n",
    "    VALUE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função Q***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_optimum(state, return_type):\n",
    "    global Q\n",
    "    # Pega as possíveis ações para o estado recebido\n",
    "    action_values = Q[state] \n",
    "    \n",
    "    # Inicializa o maior valor e a melhor ação\n",
    "    max_value = None\n",
    "    best_action = None\n",
    "    \n",
    "    # Itera sobre as ações e escolhe a melhor\n",
    "    for action, action_value in enumerate(action_values):\n",
    "        if max_value is None or action_value > max_value:\n",
    "            max_value = action_value\n",
    "            best_action = action\n",
    "            \n",
    "    # Caso o return_type seja Action, retorna a melhor ação    \n",
    "    if return_type == Return.ACTION:\n",
    "        return best_action\n",
    "    # Caso o return_type seja Value, retorna o valor da melhor ação\n",
    "    elif return_type == Return.VALUE:\n",
    "        return max_value\n",
    "    else: # Caso contrário, lança uma exceção\n",
    "        raise ValueError('Parameter return_type is not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para atualizar a matriz Q**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(state, new_state, action, reward):\n",
    "    Q[state][action] = (1-alpha)*Q_optimum(state, Return.VALUE) + alpha*(reward + gamma*Q_optimum(new_state, Return.VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para executar uma ação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(action):\n",
    "    # Executando a ação recebida\n",
    "    reward = actions[action]()\n",
    "    new_state = player\n",
    "    \n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para inicializar a matriz Q**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q():\n",
    "    for x in range(max_x):\n",
    "        for y in range(max_y):\n",
    "            Q[(x,y)] = [0.1 for _ in range(len(actions))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função para desenhar a política no tabuleiro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrows():\n",
    "    global board, Q, arrow_ids\n",
    "    policy = {}\n",
    "    \n",
    "    arrows = {\n",
    "        0: (50,70,50,30), # up\n",
    "        1: (50,30,50,70), # down\n",
    "        2: (70,50,30,50), # left\n",
    "        3: (30,50,70,50)  # right\n",
    "    }\n",
    "    \n",
    "    # Itera sobre todos os estados\n",
    "    for state, actions in Q.items():\n",
    "        best_actions = {}\n",
    "        best_index = None\n",
    "        \n",
    "        # Encontra a(s) melhor(es) ações para cada estado\n",
    "        for index, action in enumerate(actions):\n",
    "            if len(best_actions) == 0 or action > best_actions[best_index]:\n",
    "                best_index = index\n",
    "                best_actions = {}\n",
    "                best_actions[index] = action\n",
    "            elif action == best_actions[best_index]:\n",
    "                best_actions[index] = action\n",
    "        \n",
    "        policy[state] = best_actions\n",
    "        \n",
    "        # Para cada melhor ação, desenha uma seta indicando a direção\n",
    "        for action in best_actions:\n",
    "            if state == final:\n",
    "                continue\n",
    "            x0,y0,x1,y1 = arrows[action]\n",
    "            arrow_id = board.create_line(x0 + size * state[0], \n",
    "                                         y0 + size * state[1], \n",
    "                                         x1 + size * state[0],\n",
    "                                         y1 + size * state[1], \n",
    "                                         arrow=LAST)\n",
    "            arrow_ids.append(arrow_id)\n",
    "    return policy # Retorna a política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função principal**\n",
    "\n",
    "Executa o algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import threading\n",
    "\n",
    "def main():\n",
    "    global player, Q, restart, r, me, board, arrow_ids\n",
    "    \n",
    "    # Faz a thread dormir por 1 segundo\n",
    "    time.sleep(1)\n",
    "    # Executa a função para inicializar Q\n",
    "    initialize_Q()\n",
    "    # Inicializa o estado atual com a posição do agente\n",
    "    state = player\n",
    "    # Laço sem condição de parada\n",
    "    while True:\n",
    "        # Define a melhor ação a ser executada para o estado atual\n",
    "        action = Q_optimum(state, Return.ACTION)\n",
    "        # Executa a ação no agente e recebe o novo estado e a recompensa pela ação\n",
    "        new_state, reward = execute(action)\n",
    "        # Atualiza a matriz Q com as informações do novo estado e da recompensa\n",
    "        update_Q(state, new_state, action, reward)\n",
    "        # Atualiza o estado atual\n",
    "        state = new_state\n",
    "        # Desenha a política no tabuleiro\n",
    "        draw_arrows()\n",
    "       \n",
    "        # Se a flag de reinicio estiver ativa reinicia os parâmetros do programa\n",
    "        if restart:\n",
    "            time.sleep(.5)\n",
    "            player = (0,1)\n",
    "            state = player\n",
    "            restart = False\n",
    "            for arrow_id in arrow_ids:\n",
    "                board.delete(arrow_id)\n",
    "            arrow_ids = []\n",
    "            x0 = player[0]*size + size / 2 - r\n",
    "            y0 = player[1]*size + size / 2 - r\n",
    "            x1 = player[0]*size + size / 2 + r\n",
    "            y1 = player[1]*size + size / 2 + r\n",
    "            board.coords(me, x0, y0, x1, y1)\n",
    "            \n",
    "        time.sleep(.2)\n",
    "        \n",
    "t = threading.Thread(target=main)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "begin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
